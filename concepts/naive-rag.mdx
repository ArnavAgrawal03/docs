---
title: 'Introduction to RAG'
description: 'An overview of Retrieval Augmented Generation with Vector Similarity Search'
---

## What is RAG?

RAG stands for **R**etrieval **A**ugmented **G**eneration. It is a set of tools and techniques that allow us to provided additional context to an LLM given a query. 

For example, let's say you're creating an app that helps users assemble furniture. If the user is stuck on a particular step, they may want to upload a picture of their current progress and ask follow-up questions to a chatbot. RAG would allow you to take that picture, as well as the user query, and then search over a pre-ingested knowledge base - a set of user manuals for different types of furniture, for instance - and *augment* the query with context from that knowledge base. So, instead of the response being *"oh, it looks like you've screwed the rear leg backwards"*, it would be something like *"seems like you're assembling chair CX-184. You may have skipped step 8 in the assembly process, since the rear leg is screwed backwards. Here is a step-by-step solution from the assembly guide: ..."*. 

Note how both answers recognized the issue correctly, but since the LLM had additional context in the second answer, it was also able to provide a solution and more specific details. That's the jist of RAG - LLMs are generally **more helpful**, and **hallucinate less** when provided with context surrounding a query. 

While the core concept itself is simple, the complexity arises in *how* we can effectively retrieve the correct information. In the following sections, we explain one way to effectively perform RAG based on the concept of vector embeddings and similarity search (we'll explain what these mean!). 

<Note>
In reality, DataBridge uses a combination of different RAG techniques to achieve the best solution. We intend to talk about each of the techniques we implement in the [concepts](/concepts/) section of our documentation. If you're looking for a particular RAG technique, such as [ColPali](/concepts/colpali.mdx) or [Knowledge Graphs](/concepts/knowledge-graphs.mdx), you'll find it there. In this explainer, however, we'll restrict ourselves to talk about single-vector search based retrieval.
</Note>

## How does RAG work?

RAG roughly consists of 4 actions: i) *ingesting* knowledge - such as code documentation, textbooks, or product catalogues - ii) *retrieving* relevant chunks from said knowledge, iii) using the retrieved information to *augment* the user query into a better prompt, and iv) *generating* a model response from the prompt. 

### Ingest

In order to help add context to a prompt, we first need that context to exist. This is what ingestion helps with. Ingestion is the process of converting your knowledge base into a format that's optimized for retrieval. This typically involves three key steps: chunking, embedding, and indexing.

**Chunking** involves breaking down documents into smaller, manageable pieces. While LLMs have context windows that can handle thousands of tokens, we want to retrieve only the most relevant information for a given query. Chunking strategies vary based on the content type - code documentation might be chunked by function or class, while textbooks might be chunked by section or paragraph. The ideal chunk size balances granularity (smaller chunks for precise retrieval) with context preservation (larger chunks for maintaining semantic meaning).

```pseudocode
FUNCTION ChunkDocument(document)
    // Different chunking strategies for different document types
    IF document.type == "code"
        chunks = SplitByFunction(document)
    ELSE IF document.type == "article"
        chunks = SplitByParagraph(document)
    ELSE
        chunks = SplitBySentenceWithOverlap(document, windowSize=500, overlap=100)
    
    FOR EACH chunk IN chunks
        chunk.metadata = {
            source: document.title,
            type: document.type,
            position: chunk.index
        }
    
    RETURN chunks
END FUNCTION
```

**Embedding** transforms these text chunks into vector representations - essentially converting semantic meaning into mathematical space. This is done using embedding models that distill the essence of text into dense vectors. The [math and ML behind embeddings](https://www.3blue1brown.com/lessons/gpt#embedding) is really interesting. They have a [long history](https://en.wikipedia.org/wiki/Word_embedding) of development - with origins as old as 1957. Over time, models that produce word embeddings have gone through mulitple iterations - different domains, novel neural network architectures, as well as different training paradigms. 

```pseudocode
FUNCTION EmbedChunks(chunks, embeddingModel)
    embeddedChunks = []
    
    FOR EACH chunk IN chunks
        vector = embeddingModel.embed(chunk.content)
        embeddedChunks.APPEND({
            content: chunk.content,
            vector: vector,
            metadata: chunk.metadata
        })
    
    RETURN embeddedChunks
END FUNCTION
```

**Indexing** organizes these vectors for efficient similarity search. Options range from exact nearest-neighbor algorithms to approximate methods like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index). Vector databases like Pinecone, Weaviate, or Qdrant handle this complexity, optimizing for the trade-off between search speed and accuracy.

```pseudocode
FUNCTION IndexEmbeddings(embeddedChunks, vectorDB)
    FOR EACH item IN embeddedChunks
        vectorDB.upsert(
            id: GenerateUUID(),
            vector: item.vector,
            metadata: {
                content: item.content,
                source: item.metadata.source,
                type: item.metadata.type,
                position: item.metadata.position
            }
        )
    
    // Build optimized index structure
    vectorDB.buildIndex(
        indexType: "HNSW",
        parameters: {
            m: 16,  // connections per layer
            efConstruction: 200  // build-time accuracy vs. speed tradeoff
        }
    )
    
    RETURN success
END FUNCTION
```

The efficiency of your RAG system hinges on these ingestion decisions. Poorly chunked documents or low-quality embeddings can lead to irrelevant retrievals, while an inefficient index structure might impact latency. At DataBridge, we've optimized our ingestion pipeline through extensive experimentation with chunk sizes (typically 500-1000 tokens), embedding models benchmarked for domain-specific content, and vector index configurations tuned for our retrieval patterns.

### Retrieve

### Augment 

### Generate

## How can we implement RAG?

