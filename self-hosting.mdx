---
title: "Installation"
description: "Install Morphik on your own infrastructure"
---

For users who need to run Morphik on their own infrastructure, we provide two installation options: Direct Installation and Docker.

<Tabs>
  <Tab title="Self Host - Direct Installation (Advanced)">
    ## Direct Installation

    <Steps>
      <Step title="Prerequisites">
        <Accordion title="Python">
          Please ensure that you have Python 3.12 installed on your machine. Guides for installing Python can be found on the [Python website](https://www.python.org/downloads/release/python-3129/).
        </Accordion>
        <Accordion title="PostgreSQL">
          Morphik requires PostgreSQL with the pgvector extension for vector storage and similarity search capabilities. Follow the installation instructions for your operating system:

          <Tabs>
            <Tab title="macOS">
              On macOS, you can use Homebrew to install PostgreSQL and pgvector:

              ```bash
              brew install postgresql@14
              brew install pgvector
              ```

              Start the PostgreSQL service:

              ```bash
              brew services start postgresql@14
              ```

              Create a database and user for Morphik:

              ```bash
              createdb morphik
              createuser -s postgres
              ```

              These commands create a database named "morphik" and a superuser named "postgres" that the application will use to connect.
            </Tab>
            <Tab title="Ubuntu/Debian">
              Install PostgreSQL from the official repositories.**We recommend version 14** . Other versions may work, but haven't been extensively tested\!

              ```bash
              sudo apt update
              sudo apt install postgresql postgresql-contrib
              ```

              Install pgvector:

              ```bash
              sudo apt install postgresql-14-pgvector
              ```

              Start and enable the PostgreSQL service:

              ```bash
              sudo systemctl start postgresql
              sudo systemctl enable postgresql
              ```

              Create a database and user for Morphik:

              ```bash
              sudo -u postgres createdb morphik
              sudo -u postgres createuser -s postgres
              ```
            </Tab>
            <Tab title="Windows">
              1. Download and install PostgreSQL from the [official website](https://www.postgresql.org/download/windows/).
              2. During installation, make note of the password you set for the postgres user.
              3. Install pgvector:
                 - Open pgAdmin (installed with PostgreSQL)
                 - Connect to your PostgreSQL server
                 - Right-click on "Extensions" and select "Create \> Extension"
                 - Select "pgvector" from the dropdown and click "Save"
              4. Create a database for Morphik:
                 - Right-click on "Databases" and select "Create \> Database"
                 - Name it "morphik" and click "Save"
            </Tab>
          </Tabs>
          After installation, verify that PostgreSQL is running correctly:

          ```bash
          psql -U postgres -c "SELECT version();"
          ```

          You should see the following output:

          ```
           version                                                            
          ------------------------------------------------------------------------------------------------------------------------------
          <Your postgres version and some compilation details>
          ```
        </Accordion>
        <Accordion title="Additional Dependencies">
          Some system-level dependencies might be required for processing various document types:

          <Tabs>
            <Tab title="macOS">
              ```bash
              # Install via Homebrew
              brew install poppler tesseract libmagic
              ```
            </Tab>
            <Tab title="Ubuntu/Debian">
              ```bash
              # Install via apt
              sudo apt-get update
              sudo apt-get install -y poppler-utils tesseract-ocr libmagic-dev
              ```
            </Tab>
            <Tab title="Windows">
              For Windows, you may need to install these dependencies manually:

              1. **Poppler**: Download from [poppler for Windows](https://github.com/oschwartz10612/poppler-windows/releases/)
              2. **Tesseract**: Download the installer from [UB Mannheim](https://github.com/UB-Mannheim/tesseract/wiki)
              3. **libmagic**: This is included in the python-magic-bin package which will be installed with pip
            </Tab>
          </Tabs>
          If you encounter database initialization issues within Docker, you may need to manually initialize the schema:

          ```bash
          psql -U postgres -d morphik -a -f init.sql
          ```
        </Accordion>
        <Accordion title="Docker (required for background services)">
          Docker is used to spin up auxiliary services automatically (e.g., a
          local Redis container for Morphik's task queue). Install Docker
          Desktop (macOS/Windows) or the Docker engine (Linux) and make sure the
          daemon is running.
        </Accordion>
        <Accordion title="Optional: Running Local Models with Ollama">
          <Tabs>
            <Tab title="macOS">
              If you want to run models locally instead of using cloud
              providers, first install Ollama using Homebrew:

              ```bash
              brew install ollama
              ```

              Start Ollama:

              ```bash
              ollama serve
              ```

              Pull a model to use with Morphik (in a new terminal):

              ```bash
              ollama pull llama3.2
              ```
            </Tab>
            <Tab title="Linux">
              Install Ollama using the official installation script:

              ```bash
              curl -fsSL https://ollama.com/install.sh | sh
              ```

              Start Ollama:

              ```bash
              ollama serve
              ```

              Pull a model to use with Morphik (in a new terminal):

              ```bash
              ollama pull llama3.2
              ```
            </Tab>
            <Tab title="Windows">
              1. Download the Ollama installer from the [official website](https://ollama.com/download/windows).
              2. Run the installer and follow the on-screen instructions.
              3. After installation, Ollama will start automatically.
              4. Open Command Prompt or PowerShell and pull a model:

                 ```bash
                 ollama pull llama3.2
                 ```
            </Tab>
          </Tabs>
        </Accordion>
      </Step>

      <Step title="Cloning the Repository">
        To get started with Morphik, we need to first setup the server. This involves cloning the [repository](https://github.com/morphik-org/morphik-core/), installing the dependencies, and the running the server. You are just a few steps away from accurate, agentic RAG over your multi-modal data\!

        First, let's clone the repository from GitHub.

        ```bash
        git clone https://github.com/morphik-org/morphik-core.git
        ```

        After cloning the repository, navigate into the `morphik-core` folder.

        ```bash
        cd morphik-core
        ```
      </Step>

      <Step title="Setting Up the Environment">
        <h4>Installing Python Dependencies</h4>

        <Note>
          While it is not required, we highly recommend using a virtual environment to ensure dependencies from other projects do not conflict with Morphik. You may use managers like `uv` or `poetry`, but for this guide, we will use the built-in `venv` module.

          Morphik currently supports Python 3.12 as the latest version. We recommend using Python 3.12 for optimal compatibility.
        </Note>
        You don't need to create a virtual-env or run `uv sync` yourself—the
        installer script will handle everything, including installing **uv** if
        it's missing. If you prefer a manual setup at some point, install uv
        globally (`pip install --upgrade uv`) and run `uv sync` to recreate the
        environment, but this is entirely optional.

        uv reads the `pyproject.toml` / `uv.lock` files, so all dependencies
        (including optional ones like `unstructured`) are declared in one place.

        <Note>
          If you need additional NLTK data you can still download it afterwards:

          ```bash
          python -m nltk.downloader averaged_perceptron_tagger punkt
          ```
        </Note>

        ---

      </Step>

      <Step title="Run the Installer Script">
        From the project root, run the one-liner below – it auto-detects your
        platform (macOS or Linux), sets any required build flags, installs
        dependencies, and starts the server:

        ```bash
        # Make the script executable
        chmod +x install_and_start.sh

        # Run the installer
        ./install_and_start.sh
        ```

        The script will:

        1. Create & activate a `.venv` with **uv**
        2. Install `colpali-engine` (knowledge-graph generation)
        3. Build `llama-cpp-python` (Metal acceleration on Apple Silicon)
        4. Launch the server via `uv run start_server.py`

        <Note>
          You only need to run `./install_and_start.sh` the first time to set up
          the environment. For future sessions, activate your project directory
          and simply start the server with:

          ```bash
          uv run start_server.py
          ```
        </Note>
      </Step>

      <Step title="Setting up the Server Parameters">
        At this point, you may want to customize the server - such as use a different model, enable or disable certain features, etc. - you can do so by editing the `morphik.toml` file. 
        
        Morphik now uses a registered models approach, which allows you to define hundreds of different models in one place and reference them throughout your configuration. This makes it easy to mix and match models based on your needs (e.g., smaller models for simpler tasks). You can find more details about configuration [here](/configuration).

        The installer copies `.env.example` to `.env` automatically if it's
        missing. After the script finishes, open `.env` to add any API keys (e.g.
        `OPENAI_API_KEY`) or secrets you need. You can tweak `morphik.toml`
        anytime to switch completion/embedding models, adjust chunking, or enable
        advanced features.
      </Step>

      <Step title="Launching the Server">
        You are now ready to launch the Morphik server\! Just run the following command to start the server.

        ```bash
        uv run start_server.py
        ```

        You should see the following output:

        ```
        INFO:     Started server process [15169]
        INFO:     Waiting for application startup.
        INFO:     Application startup complete.
        INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)
        ```

        This means that the server is running on [http://localhost:8000](http://localhost:8000). You can now interact with the server using the API or the Python SDK.
      </Step>
    </Steps>
  </Tab>
  <Tab title="Self Host - Docker (Advanced)">
    ## Using Docker

    Morphik provides a streamlined Docker-based setup that includes all necessary components: the core API, PostgreSQL with pgvector, and Ollama for AI models.

    <Note>
      If you are using an Apple Silicon (M-series) Mac, we highly recommend using the Direct Installation method instead of Docker. GPU passthrough is not supported via Docker on Apple Silicon, which can significantly impact performance.
    </Note>

    ### Prerequisites

    - Docker and Docker Compose installed on your system
    - At least 10GB of free disk space (for models and data)
    - 8GB\+ RAM recommended

    ### Quick Start

    **Option&nbsp;A — Run the pre-built image**

    This is the recommended method for most users. It uses an interactive script to set up and run Morphik with Docker Compose.

    All you need is a single command:

    ```bash
    curl -sSL https://raw.githubusercontent.com/morphik-org/morphik-core/main/install_docker.sh | bash
    ```

    This script will:
    1.  Check for Docker and Docker Compose.
    2.  Warn you if you are on an Apple Silicon Mac and give you the option to proceed.
    3.  Download the necessary `docker-compose.run.yml`.
    4.  Create a `.env` file and ask you for your OpenAI API Key.
    5.  Optionally allow you to customize the `morphik.toml` configuration before starting.
    6.  Start all the services (`morphik`, `worker`, `postgres`, and `redis`).

    The server will then be available at [http://localhost:8000](http://localhost:8000). To use local models with Ollama, you can run `docker compose -f docker-compose.run.yml --profile ollama up -d` after the installation script has completed.

    ---

    **Option&nbsp;B — Develop from source (build locally)**
    <Note>
    This option uses `docker-compose.yml` to build the image from the source code in this repository. The `start.sh` script simplifies this process.
    </Note>
    ```bash
    # Clone and enter the repo
    git clone https://github.com/morphik-org/morphik-core.git
    cd morphik-core

    # Start everything (Postgres, Redis, Morphik) with sane defaults
    chmod +x start.sh
    ./start.sh
    ```

    Once the containers are up visit [http://localhost:8000/docs](http://localhost:8000/docs).

    Need local models via Ollama?  Skip `start.sh` and run:

    ```bash
    docker compose --profile ollama up --build -d
    ```

    ---

    ### Advanced configuration

    • **Change models** – edit `morphik.docker.toml` (for dev) or the file you mounted in Option A.  The most common edits are:
      - switch `[completion] model` to `openai_gpt4-1` for full GPT-4, or any key in `[registered_models]`.
      - lower `dimensions` back to 768 if you use a smaller embedding.

    • **Change ports / database** – edit `.env` (HOST, PORT, POSTGRES_URI).

    • **Persist data** – by default volumes declared in `docker-compose.yml` persist Postgres and storage across restarts.  For the pre-built image add `-v my_pg:/var/lib/postgresql/data` etc.

    ---

    ### Configuration

    The default configuration works out of the box and includes:

    - PostgreSQL with pgvector for document storage
    - Ollama for AI models (embeddings and completions)
    - Local file storage
    - Basic authentication

    You can customize your setup by creating a `.env` file:

    ```bash
    JWT_SECRET_KEY=your-secure-key-here  # Important: Change in production
    OPENAI_API_KEY=sk-...                # Only if using OpenAI
    HOST=0.0.0.0                         # Leave as is for Docker
    PORT=8000                            # Change if needed
    ```

    ### Accessing Services

    - Morphik API: http://localhost:8000
    - API Documentation: http://localhost:8000/docs
    - Health Check: http://localhost:8000/health

    ### Troubleshooting

    1. **Service Won't Start**

       ```bash
       # View all logs
       docker compose logs
       
       # View specific service logs
       docker compose logs morphik
       docker compose logs postgres
       docker compose logs ollama
       ```
    2. **Database Issues**
       - Check PostgreSQL is healthy: `docker compose ps`
       - Verify database connection: `docker compose exec postgres psql -U morphik -d morphik`

    3. **Model Download Issues**
       - Check Ollama logs: `docker compose logs ollama`
       - Ensure enough disk space for models
       - Try restarting Ollama: `docker compose restart ollama`
       - Verify that your Redis and Ollama configuration in `morphik.toml` matches your deployment:
         - For Redis in Docker, use `host = "redis"` (not "localhost")
         - If running Ollama in Docker, use `http://ollama:11434` endpoint
         - If running Ollama locally but Morphik in Docker, use `http://host.docker.internal:11434`
    4. **Ollama Memory Issues**
       - If you see errors like "model requires more system memory than is available" in the logs:
         - Increase Docker memory allocation in Docker Desktop:
           1. Open Docker Desktop application
           2. Click on Settings (gear icon)
           3. Go to "Resources" section
           4. Increase memory allocation to at least 6GB (8GB recommended)
           5. Click "Apply & Restart"
         - Alternatively, modify `morphik.toml` to use a smaller model
         - Or switch to OpenAI API by uncommenting the OpenAI sections in `morphik.toml`
       - For knowledge graph creation specifically, you can switch from Ollama to OpenAI:

         ```toml
         # Change this section in morphik.toml
         [graph]
         provider = "openai"
         model_name = "gpt-4o-mini"
         enable_entity_resolution = true
         ```
    5. **Performance Issues**
       - Monitor resources: `docker stats`
       - Ensure sufficient RAM (8GB\+ recommended)
       - Check disk space: `df -h`
  </Tab>
</Tabs>