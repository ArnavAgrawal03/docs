---
title: 'How do I scale search to many documents?'
description: 'Approaches for indexing large datasets'
---

When your corpus grows, efficient indexing and caching become critical. Morphik's batch ingestion APIs let you process data asynchronously:

```python
from morphik import Morphik
client = Morphik()

client.batch_ingest(["/data/drop1", "/data/drop2"])
```

After ingestion, build caches targeting specific subsets of documents:

```python
client.create_cache(name="manual-cache", filters={"product":"docs"})
```

Caches isolate results, so queries remain fast even with millions of documents. See [batch_ingest](/python-sdk/batch_get_documents) and [create_cache](/python-sdk/create_cache) for details.
